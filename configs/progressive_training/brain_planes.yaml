# @package _global_

# example progressing learning of some experiment:
# python src/train.py -m hydra=progressive_training experiment=brain_planes

defaults:
  - override /hydra/sweeper: progressive_training

optimized_metric: "val/acc_best"

hydra:
  mode: "MULTIRUN" # set hydra to multirun by default if this config is attached

  sweeper:
    # number of training epochs (trainer.max_epochs value)
    epochs: ${trainer.max_epochs}

    # number of stages. In each stage we will run epochs/stages epoch
    stages: 2

    # which checkpoint to take for next stage: last or best
    ckpt: "best"
    #    runs_per_stage: 3

    # define parameters for progressive training.
    # Each parameter should have three values in a list - [min, max, type]
    params:
      data.input_size:
        type: "linear"
        #        min: [55, 80]
        min: [80, 120]
        max: [165, 240]
        #        max: [200, 300]
        dtype: "int"
      data.rand_augment_magnitude:
        type: "linear"
        min: 5
        max: 10
        dtype: "int"
      #      model.net_spec.dropout:
      #        type: "linear"
      #        min: 0.1
      #        max: 0.3
      #        dtype: "float"
      trainer.accumulate_grad_batches:
        type: "step"
        steps:
          - stage: 1
            value: 2
      data.batch_size:
        type: "step"
        steps:
          - stage: 1
            value: 16
#      model.optimizer.lr:
#        type: "step"
#        steps:
#          - stage: 1
#            value: 1e-05

callbacks:
  early_stopping:
