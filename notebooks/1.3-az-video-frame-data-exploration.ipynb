{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0f37bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import pathlib\n",
    "import shutil\n",
    "from math import ceil, sqrt\n",
    "from pathlib import Path\n",
    "from typing import Callable, List, Optional, Tuple\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import rootutils\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from skimage.metrics import structural_similarity\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from torch import Tensor\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Dataset\n",
    "from torchvision.io import read_image\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# from IPython.display import set_matplotlib_formats\n",
    "# set_matplotlib_formats('png')\n",
    "\n",
    "root = rootutils.setup_root(search_from=\".\", indicator=\".project-root\", pythonpath=True)\n",
    "\n",
    "from src.data.components.dataset import (\n",
    "    FetalBrainPlanesDataset,\n",
    "    USVideosDataset,\n",
    "    USVideosFrameDataset,\n",
    ")\n",
    "from src.data.components.transforms import Affine, HorizontalFlip\n",
    "from src.data.utils.utils import show_numpy_images, show_pytorch_images\n",
    "from src.models.fetal_module import FetalLitModule\n",
    "from src.models.quality_module import QualityLitModule\n",
    "\n",
    "path = root / \"data\"\n",
    "root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc97462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_file = root / \"logs\" / \"train\" / \"runs\" / \"2023-03-07_05-55-06\"\n",
    "\n",
    "checkpoint = sorted(checkpoint_file.glob(\"checkpoints/epoch_*.ckpt\"))[-1]\n",
    "rnn = QualityLitModule.load_from_checkpoint(str(checkpoint))\n",
    "# disable randomness, dropout, etc...\n",
    "rnn.eval()\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6512c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoQualityDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str,\n",
    "        dataset_name: str = \"US_VIDEOS\",\n",
    "        train: bool = True,\n",
    "        seq_len: int = 32,\n",
    "        seq_step: int = None,\n",
    "        reverse: bool = False,\n",
    "        transform: bool = False,\n",
    "        normalize: bool = False,\n",
    "        target_transform: Callable | None = None,\n",
    "        label_transform: Callable | None = None,\n",
    "    ):\n",
    "        self.train = train\n",
    "        self.dataset_dir = Path(data_dir) / dataset_name / \"data\"\n",
    "        self.data_dir = self.dataset_dir / (\"train\" if self.train else \"test\")\n",
    "        self.seq_len = seq_len\n",
    "        self.seq_step = seq_step\n",
    "        self.reverse = reverse\n",
    "        self.transform = transform\n",
    "        self.clips = self.load_clips()\n",
    "        self.normalize = normalize\n",
    "        self.std_mean = torch.load(f\"{self.dataset_dir}/std_mean.pt\")\n",
    "\n",
    "        self.target_transform = target_transform\n",
    "        self.label_transform = label_transform\n",
    "\n",
    "    def load_clips(self):\n",
    "        clips = []\n",
    "        print(f\"video_paths: {len(list(self.data_dir.iterdir()))}\")\n",
    "        for video_path in sorted(self.data_dir.iterdir()):\n",
    "            print(f\"video_path: {video_path.name}\")\n",
    "            transforms = [transform_path.name for transform_path in sorted(video_path.iterdir())]\n",
    "\n",
    "            transform_path = sorted(video_path.iterdir())[0]\n",
    "            logits, quality, _ = torch.load(transform_path)\n",
    "\n",
    "            seq_len = self.seq_len or len(quality)\n",
    "            seq_step = self.seq_step or max(1, ceil(seq_len / 2))\n",
    "\n",
    "            for from_idx in range(0, len(quality) - seq_len + 1, seq_step):\n",
    "                to_idx = from_idx + seq_len\n",
    "                clips.append((video_path.name, transforms, from_idx, to_idx, False))\n",
    "                if self.train and self.reverse:\n",
    "                    clips.append((video_path.name, transforms, from_idx, to_idx, True))\n",
    "\n",
    "        return pd.DataFrame(clips, columns=[\"Video\", \"Transforms\", \"From\", \"To\", \"Flip\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.clips)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self):\n",
    "            raise IndexError(f\"list index {idx} out of range\")\n",
    "\n",
    "        if isinstance(idx, torch.Tensor):\n",
    "            idx = idx.item()\n",
    "\n",
    "        transforms = self.clips.Transforms[idx]\n",
    "        transform_idx = torch.randint(0, len(transforms), ()) if (self.train and self.transform) else 0\n",
    "\n",
    "        video = self.data_dir / self.clips.Video[idx] / transforms[transform_idx]\n",
    "        logits, quality, preds = torch.load(video)\n",
    "\n",
    "        from_idx = self.clips.From[idx]\n",
    "        to_idx = self.clips.To[idx]\n",
    "        x = logits[from_idx:to_idx]\n",
    "        y = quality[from_idx:to_idx]\n",
    "        p = preds[from_idx:to_idx]\n",
    "\n",
    "        if self.clips.Flip[idx]:\n",
    "            x = torch.flip(x, dims=[0])\n",
    "            y = torch.flip(y, dims=[0])\n",
    "            p = torch.flip(p, dims=[0])\n",
    "\n",
    "        if self.normalize is not None:\n",
    "            x = (x - self.std_mean[1]) / self.std_mean[0]\n",
    "\n",
    "        if self.target_transform:\n",
    "            y = self.target_transform(y)\n",
    "        if self.label_transform:\n",
    "            p = self.label_transform(p)\n",
    "\n",
    "        return x, y, p\n",
    "\n",
    "\n",
    "dataset = VideoQualityDataset(\n",
    "    data_dir=path,\n",
    "    dataset_name=\"US_VIDEOS_tran_250_playful-haze-2111\",\n",
    "    train=False,\n",
    "    seq_len=128,\n",
    "    #     seq_step=None,\n",
    "    #     reverse=False,\n",
    "    #     transform=False,\n",
    "    #     normalize=False\n",
    ")\n",
    "# torch.Size([472])\n",
    "# torch.Size([329])\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    print(f\"{dataset.clips.From[i]} {dataset.clips.To[i]}\")\n",
    "\n",
    "print(len(dataset))\n",
    "for i in range(len(dataset)):\n",
    "    print(dataset[i][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b6fc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))\n",
    "for i in range(len(dataset)):\n",
    "    print(dataset[i][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2382c4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=8,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "for d in dl:\n",
    "    print(len(d))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
