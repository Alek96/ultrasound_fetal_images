{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import shutil\n",
    "from math import ceil, sqrt\n",
    "from pathlib import Path\n",
    "from typing import Callable, List, Optional, Tuple\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import pyrootutils\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as F\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from torch.utils.data import ConcatDataset, Dataset\n",
    "from torchvision.io import read_image\n",
    "from tqdm import tqdm\n",
    "\n",
    "root = pyrootutils.setup_root(search_from=\".\", indicator=\".project-root\", pythonpath=True)\n",
    "\n",
    "from src.models.fetal_module import FetalLitModule\n",
    "\n",
    "root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_files = [\n",
    "    root\n",
    "    / \"logs\"\n",
    "    / \"train\"\n",
    "    / \"multiruns\"\n",
    "    / \"2023-02-03_15-13-23\"\n",
    "    / \"68\"\n",
    "    / \"checkpoints\"\n",
    "    / \"epoch_015.ckpt\",\n",
    "    root\n",
    "    / \"logs\"\n",
    "    / \"train\"\n",
    "    / \"multiruns\"\n",
    "    / \"2023-02-03_15-13-23\"\n",
    "    / \"118\"\n",
    "    / \"checkpoints\"\n",
    "    / \"epoch_011.ckpt\",\n",
    "    root\n",
    "    / \"logs\"\n",
    "    / \"train\"\n",
    "    / \"multiruns\"\n",
    "    / \"2023-02-03_15-13-23\"\n",
    "    / \"171\"\n",
    "    / \"checkpoints\"\n",
    "    / \"epoch_012.ckpt\",\n",
    "    root\n",
    "    / \"logs\"\n",
    "    / \"train\"\n",
    "    / \"multiruns\"\n",
    "    / \"2023-02-03_15-13-23\"\n",
    "    / \"154\"\n",
    "    / \"checkpoints\"\n",
    "    / \"epoch_010.ckpt\",\n",
    "    root\n",
    "    / \"logs\"\n",
    "    / \"train\"\n",
    "    / \"multiruns\"\n",
    "    / \"2023-02-03_15-13-23\"\n",
    "    / \"173\"\n",
    "    / \"checkpoints\"\n",
    "    / \"epoch_012.ckpt\",\n",
    "]\n",
    "\n",
    "models = []\n",
    "\n",
    "for file in checkpoint_files:\n",
    "    model = FetalLitModule.load_from_checkpoint(str(file))\n",
    "    # disable randomness, dropout, etc...\n",
    "    model.eval()\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = root / \"data\" / \"US_VIDEOS\"\n",
    "shutil.rmtree(path / \"labeled\", ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = T.Compose(\n",
    "    [\n",
    "        T.ToTensor(),\n",
    "        T.Grayscale(),\n",
    "        T.Resize((55, 80)),\n",
    "        T.ConvertImageDtype(torch.float32),\n",
    "    ]\n",
    ")\n",
    "\n",
    "labels = [\n",
    "    \"Other\",\n",
    "    \"Maternal cervix\",\n",
    "    \"Fetal abdomen\",\n",
    "    \"Fetal brain\",\n",
    "    \"Fetal femur\",\n",
    "    \"Fetal thorax\",\n",
    "]\n",
    "\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "logsoftmax = torch.nn.LogSoftmax(dim=1)\n",
    "\n",
    "\n",
    "def label_videos(path: pathlib.Path):\n",
    "    videos_path = path / \"videos\"\n",
    "    images_path = path / \"labeled\"\n",
    "    videos = len(list(videos_path.iterdir()))\n",
    "    for i, video_path in enumerate(videos_path.iterdir()):\n",
    "        label_video(video_path, images_path, i + 1, videos)\n",
    "\n",
    "\n",
    "#         break\n",
    "\n",
    "\n",
    "def frame_iter(capture, description):\n",
    "    def iterator():\n",
    "        while capture.grab():\n",
    "            yield capture.retrieve()[1]\n",
    "\n",
    "    return tqdm(\n",
    "        iterator(),\n",
    "        desc=description,\n",
    "        total=int(capture.get(cv2.CAP_PROP_FRAME_COUNT)),\n",
    "    )\n",
    "\n",
    "\n",
    "def label_video(video_path: pathlib.Path, images_path: pathlib.Path, it: int, videos: int):\n",
    "    if not video_path.exists():\n",
    "        print(f\"path {video_path} not exist\")\n",
    "\n",
    "    vidcap = cv2.VideoCapture(str(video_path))\n",
    "    for i, frame in enumerate(frame_iter(vidcap, f\"label video {it}/{videos}\")):\n",
    "        label = label_frame(frame)\n",
    "        img_path = images_path / video_path.stem / label / (\"frame%d.jpg\" % i)\n",
    "        if not img_path.parent.exists():\n",
    "            img_path.parent.mkdir(parents=True)\n",
    "        cv2.imwrite(str(img_path), frame)\n",
    "\n",
    "    #         if i + 1 == 36:\n",
    "    #             break;\n",
    "\n",
    "    count_images(images_path / video_path.stem)\n",
    "\n",
    "\n",
    "def label_frame(frame):\n",
    "    with torch.no_grad():\n",
    "        frame = PIL.Image.fromarray(frame)\n",
    "        frame = transforms(frame)\n",
    "        frame = frame.unsqueeze(0)\n",
    "\n",
    "        ys = None\n",
    "        for model in models:\n",
    "            with torch.no_grad():\n",
    "                y = model(frame)\n",
    "                y = softmax(y)\n",
    "                pred = torch.argmax(y, dim=1).item()\n",
    "                if ys is None:\n",
    "                    ys = y\n",
    "                else:\n",
    "                    ys += y\n",
    "\n",
    "        pred = torch.argmax(ys, dim=1).item()\n",
    "        #         print(f\"pred: {pred}, {softmax(ys)[0][3]:.4f}\")\n",
    "        #         print(softmax(ys))\n",
    "        #         print(pred)\n",
    "\n",
    "        return labels[pred]\n",
    "\n",
    "\n",
    "def count_images(images_path: pathlib.Path):\n",
    "    count = {}\n",
    "    for label in labels:\n",
    "        count[label] = 0\n",
    "    for label_dir in images_path.iterdir():\n",
    "        count[label_dir.name] = len(list(label_dir.iterdir()))\n",
    "    print(count)\n",
    "\n",
    "\n",
    "torch.set_printoptions(precision=2)\n",
    "path = root / \"data\" / \"US_VIDEOS\"\n",
    "label_videos(path)\n",
    "torch.set_printoptions(profile=\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    \"Other\",\n",
    "    \"Maternal cervix\",\n",
    "    \"Fetal abdomen\",\n",
    "    \"Fetal brain\",\n",
    "    \"Fetal femur\",\n",
    "    \"Fetal thorax\",\n",
    "]\n",
    "\n",
    "\n",
    "def count_all_images(images_path: pathlib.Path):\n",
    "    count = {}\n",
    "    for label in labels:\n",
    "        count[label] = 0\n",
    "    for video_dir in images_path.iterdir():\n",
    "        for label_dir in video_dir.iterdir():\n",
    "            count[label_dir.name] += len(list(label_dir.iterdir()))\n",
    "    return count\n",
    "\n",
    "\n",
    "path = root / \"data\" / \"US_VIDEOS\" / \"labeled\"\n",
    "images = count_all_images(path)\n",
    "for key, item in images.items():\n",
    "    print(f\"{key}: {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class USVideosDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str,\n",
    "        transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None,\n",
    "    ):\n",
    "        data_dir = Path(data_dir) / \"US_VIDEOS\" / \"labeled\"\n",
    "        images = self.find_images(data_dir)\n",
    "        self.items = []\n",
    "        for key, items in images.items():\n",
    "            self.items.extend([(str(item), key) for item in items])\n",
    "\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    @staticmethod\n",
    "    def find_images(images_path: pathlib.Path):\n",
    "        images = {}\n",
    "        for video_dir in images_path.iterdir():\n",
    "            for label_dir in video_dir.iterdir():\n",
    "                label = label_dir.name\n",
    "                if label not in images:\n",
    "                    images[label] = []\n",
    "                images[label].extend(label_dir.iterdir())\n",
    "        return images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self):\n",
    "            raise IndexError(\"list index out of range\")\n",
    "\n",
    "        img_path, label = self.items[idx]\n",
    "        image = read_image(img_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "data_dir = root / \"data\"\n",
    "dataset = USVideosDataset(\n",
    "    data_dir=str(data_dir),\n",
    ")\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(imgs, tick_labels: bool = True):\n",
    "    n = ceil(sqrt(len(imgs)))\n",
    "\n",
    "    fig, axes = plt.subplots(ncols=n, nrows=n, squeeze=False, figsize=(20, 15))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i * n + j >= len(imgs):\n",
    "                continue\n",
    "\n",
    "            img, label = imgs[i * n + j]\n",
    "            img = img.detach()\n",
    "            img = F.to_pil_image(img)\n",
    "            img = F.to_grayscale(img)\n",
    "            axes[i, j].imshow(np.asarray(img), cmap=\"gray\")\n",
    "            axes[i, j].set_xlabel(label)\n",
    "            if not tick_labels:\n",
    "                axes[i, j].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show(dataset, tick_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = USVideosDataset(\n",
    "    data_dir=str(root / \"data\"),\n",
    ")\n",
    "print(len(dataset))\n",
    "\n",
    "\n",
    "def compare_image(imgs):\n",
    "    hists = []\n",
    "    for i in range(len(imgs)):\n",
    "        img_path, label = dataset.items[i]\n",
    "        img = cv2.imread(img_path)\n",
    "        hist = cv2.calcHist([img], [0], None, [256], [0, 256])\n",
    "        hists.append(hist)\n",
    "\n",
    "    for i in range(len(hists) - 1):\n",
    "        hist_diff = cv2.compareHist(hists[i], hists[i + 1], cv2.HISTCMP_BHATTACHARYYA)\n",
    "\n",
    "        template_probability_match = cv2.matchTemplate(\n",
    "            hists[i], hists[i + 1], cv2.TM_CCOEFF_NORMED\n",
    "        )[0][0]\n",
    "        template_diff = 1 - template_probability_match\n",
    "\n",
    "        c = 0\n",
    "        # Euclidean Distance between data1 and test\n",
    "        j = 0\n",
    "        while j < len(hists[i]) and j < len(hists[i + 1]):\n",
    "            c += (hists[i][j] - hists[i + 1][j]) ** 2\n",
    "            j += 1\n",
    "        c = c ** (1 / 2)\n",
    "\n",
    "        print(f\"hist {hist_diff}, template {template_diff}, L2 {c}\")\n",
    "\n",
    "        # etropia\n",
    "        # ssi -\n",
    "\n",
    "\n",
    "compare_image(dataset)\n",
    "# show(dataset, tick_labels=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
