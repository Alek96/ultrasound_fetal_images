{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0f37bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import pathlib\n",
    "import shutil\n",
    "from math import ceil, sqrt\n",
    "from pathlib import Path\n",
    "from typing import Callable, List, Optional, Tuple\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import pyrootutils\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from skimage.metrics import structural_similarity\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from torch import Tensor\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Dataset\n",
    "from torchvision.io import read_image\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "root = pyrootutils.setup_root(search_from=\".\", indicator=\".project-root\", pythonpath=True)\n",
    "\n",
    "from src.data.components.dataset import FetalBrainPlanesDataset, USVideosDataset\n",
    "from src.data.components.transforms import Affine, HorizontalFlip\n",
    "from src.data.utils.utils import group_split, show_numpy_images, show_pytorch_images\n",
    "from src.models.fetal_module import FetalLitModule\n",
    "\n",
    "data_dir = root / \"data\"\n",
    "root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc97462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoQualityDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str,\n",
    "        dataset_name: str = \"US_VIDEOS\",\n",
    "        train: bool = True,\n",
    "        window_size: int = 32,\n",
    "        transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None,\n",
    "    ):\n",
    "        self.data_dir = Path(data_dir) / dataset_name / \"data\" / (\"train\" if train else \"test\")\n",
    "        self.window_size = window_size\n",
    "        self.clips = self.load_clips()\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def load_clips(self):\n",
    "        clips = []\n",
    "        for video in sorted(self.data_dir.iterdir()):\n",
    "            _, quality = torch.load(video)\n",
    "            window_size = self.window_size or len(quality)\n",
    "            for i in range(len(quality) - window_size + 1):\n",
    "                clips.append((video.name, i, i + window_size))\n",
    "\n",
    "        return pd.DataFrame(clips, columns=[\"Video\", \"From\", \"To\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.clips)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self):\n",
    "            raise IndexError(\"list index out of range\")\n",
    "\n",
    "        video = self.data_dir / self.clips.Video[idx]\n",
    "        logits, quality = torch.load(video)\n",
    "\n",
    "        from_idx = self.clips.From[idx]\n",
    "        to_idx = self.clips.To[idx]\n",
    "        x = logits[from_idx:to_idx]\n",
    "        y = quality[from_idx:to_idx]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        if self.target_transform:\n",
    "            y = self.target_transform(y)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "dataset = VideoQualityDataset(\n",
    "    data_dir=data_dir,\n",
    "    dataset_name=\"US_VIDEOS\",\n",
    "    window_size=0,\n",
    "    train=False,\n",
    ")\n",
    "print(len(dataset))\n",
    "\n",
    "x, y = dataset[0]\n",
    "print(x.shape, x.dtype)\n",
    "print(y.shape, y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108a3ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(dataset))):\n",
    "    dataset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ffcafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_val = group_split(\n",
    "    dataset=dataset,\n",
    "    test_size=0.1,\n",
    "    groups=dataset.clips.Video,\n",
    "    random_state=42,\n",
    ")\n",
    "print(len(data_train))\n",
    "print(len(data_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591920e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=2,\n",
    ")\n",
    "\n",
    "for batch in dataloader:\n",
    "    x, y = batch\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f46ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor([[[1.0, 1.0, 1.0], [2.0, 2.0, 2.0]], [[3.0, 3.0, 3.0], [4.0, 4.0, 4.0]]])\n",
    "print(y.shape)\n",
    "print(y)\n",
    "\n",
    "y = y.reshape(-1, 3)\n",
    "print(y.shape)\n",
    "print(y)\n",
    "\n",
    "lin = torch.nn.Linear(3, 1, bias=False)\n",
    "lin.weight.data.fill_(1)\n",
    "y = lin(y)\n",
    "print(y)\n",
    "\n",
    "y.reshape(2, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
